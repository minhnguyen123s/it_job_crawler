import os
import asyncio
import json
import csv
import re
from dotenv import load_dotenv
from browser_use import Agent, Browser, ChatBrowserUse
import unicodedata

load_dotenv()
print("API KEY =", os.getenv("BROWSER_USE_API_KEY"))

def normalize_text(text: str) -> str:
    """Normalize unicode + thay m·ªçi d·∫°ng newline b·∫±ng space"""
    text = unicodedata.normalize('NFC', text)
    text = re.sub(r'\\?[\\r\\n]+', ' ', text)          # X·ª≠ l√Ω \n literal v√† \\n escaped
    text = re.sub(r'\s+', ' ', text.strip())           # Nhi·ªÅu space ‚Üí 1 space
    return text

def normalize_key(key: str) -> str:
    """Chu·∫©n h√≥a key: x√≥a space th·ª´a, s·ª≠a typo ph·ªï bi·∫øn"""
    clean = key.strip().lower()
    if "title" in clean or "job_t" in clean:
        return "job_title"
    if "compa" in clean or "name" in clean or "c√¥ng ty" in clean:
        return "company_name"
    if "sala" in clean or "l∆∞∆°ng" in clean:
        return "salary"
    if "link" in clean or "detail" in clean or "url" in clean:
        return "job_detail_link"
    return clean  # gi·ªØ nguy√™n n·∫øu kh√¥ng kh·ªõp

async def crawl_it_jobs():
    browser = Browser()
    llm = ChatBrowserUse()

    agent = Agent(
        task="""
Go to https://www.topcv.vn/tim-viec-lam-it-phan-mem

Extract exactly 10 job listings from the search results page.
Return ONLY a valid JSON array of objects. NOTHING else: no text, no markdown, no ```json, no explanation, no extra characters.

Each object MUST have exactly these 4 keys (exact spelling, no spaces around, no trailing spaces):
"job_title"
"company_name"
"salary"
"job_detail_link"

Rules nghi√™m ng·∫∑t:
- Keys exactly as above: no extra spaces, no typos like "sala y ", "compa y_ ame ", "job_title "
- All string values use double quotes "
- Escape properly: no raw newlines inside strings (replace \\n with space if any)
- Clean company_name: remove ALL extra newlines, line breaks, multiple spaces ‚Üí single space only
- Salary: gi·ªØ nguy√™n nh∆∞ trang (v√≠ d·ª•: "T·ªõi 30 tri·ªáu", "Tho·∫£ thu·∫≠n", "T·ªõi 70 tri·ªáu")
- job_detail_link: full absolute URL, bao g·ªìm https://
- Exactly 10 items if possible, or as many as available
""",
        llm=llm,
        browser=browser,
    )

    print("ƒêang ch·∫°y agent...")

    history = await agent.run()

    final = history.final_result()

    if isinstance(final, dict):
        final_text = final.get("text") or final.get("content") or str(final)
    else:
        final_text = str(final)

    print("\nüìÑ RAW RESULT:")
    print(final_text)

    # L√†m s·∫°ch to√†n b·ªô text
    clean_text = normalize_text(final_text)

    print("\nüßπ CLEANED TEXT:")
    print(clean_text)

    # T√¨m ph·∫ßn JSON array
    start = clean_text.find('[')
    end = clean_text.rfind(']') + 1

    if start == -1 or end <= start:
        print("‚ùå Kh√¥ng t√¨m th·∫•y m·∫£ng JSON h·ª£p l·ªá!")
        jobs = []
    else:
        json_str = clean_text[start:end]
        print("\nüîç JSON string sau clean:")
        print(json_str)

        try:
            parsed_jobs = json.loads(json_str)
            print(f"‚úÖ Parse tr·ª±c ti·∫øp th√†nh c√¥ng: {len(parsed_jobs)} jobs")

            # Normalize keys & values
            normalized_jobs = []
            expected_keys = {"job_title", "company_name", "salary", "job_detail_link"}

            for job in parsed_jobs:
                normalized = {}
                for k, v in job.items():
                    clean_key = normalize_key(k)
                    if clean_key in expected_keys:
                        normalized[clean_key] = v.strip() if isinstance(v, str) else v

                # Ch·ªâ gi·ªØ job n·∫øu c√≥ ƒë·ªß 4 key
                if len(normalized) == 4:
                    normalized_jobs.append(normalized)
                else:
                    print(f"B·ªè qua job thi·∫øu ho·∫∑c key kh√¥ng h·ª£p l·ªá: {job}")

            jobs = normalized_jobs
            print(f"‚úÖ Sau normalize & filter: {len(jobs)} jobs h·ª£p l·ªá")

        except json.JSONDecodeError as e:
            print(f"‚ùå L·ªói json.loads: {e}")
            print("V·ªã tr√≠ l·ªói:", e.pos)
            print("ƒêo·∫°n g·∫ßn l·ªói:", json_str[max(0, e.pos-50):e.pos+50])
            jobs = []

    # Ghi file CSV
    if jobs:
        csv_filename = "it_jobs.csv"
        with open(csv_filename, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(
                f,
                fieldnames=["job_title", "company_name", "salary", "job_detail_link"]
            )
            writer.writeheader()
            writer.writerows(jobs)
        print(f"‚úÖ ƒê√£ xu·∫•t th√†nh c√¥ng {len(jobs)} jobs v√†o file: {csv_filename}")
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ job n√†o h·ª£p l·ªá ƒë·ªÉ xu·∫•t file.")

if __name__ == "__main__":
    asyncio.run(crawl_it_jobs())